<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Drifting Ruby</title>
  <subtitle>Collection of ideas about programming, screencasts, Ruby on Rails, and more.</subtitle>
  <id>https://blog.driftingruby.com/articles</id>
  <link href="https://blog.driftingruby.com/articles"/>
  <link href="https://blog.driftingruby.com/feed.xml" rel="self"/>
  <updated>2023-10-28T21:41:00-04:00</updated>
  <author>
    <name>Dave Kimura</name>
  </author>
  <entry>
    <title>Using Ansible to Update Kamal Servers</title>
    <link rel="alternate" href="https://blog.driftingruby.com/articles/2023/10/29/using-ansible-to-update-kamal-servers.html"/>
    <id>https://blog.driftingruby.com/articles/2023/10/29/using-ansible-to-update-kamal-servers.html</id>
    <published>2023-10-28T21:41:00-04:00</published>
    <updated>2023-10-28T22:09:13-04:00</updated>
    <author>
      <name>Dave Kimura</name>
    </author>
    <content type="html">&lt;p&gt;I've been using Kamal recently and it's been a great experience. It takes a lot of the headaches
out of having to wait for a deployment to finish. Previously, I was hosting &lt;a href="https://www.driftingruby.com"&gt;Drifting Ruby&lt;/a&gt;
on AWS with Elastic Beanstalk. This was a great solution for a long time as I didn't have to
worry about the servers and it set up a lot of the networking for me. However, there were some pain points
with this process and I have been wanting to switch to something new for a while. When I first heard about
MRSK (now Kamal), I was excited! Ruby on Rails applications were getting a first class citizen deployment
solution. The networking and infrastructure side of things hasn't ever been an issue for me since I initially
came from that kind of background. However, I was initially looking for something easy to use that gave a bit
of flexibility. I didn't want to go the Heroku route since pricing was a concern early on and AWS offers Reserved
Instances which can save a significant amount.&lt;/p&gt;

&lt;p&gt;One of the drawbacks with Kamal is that it doesn't have a built in way to update the servers. This is where
Ansible comes in. Ansible is a tool that allows you to automate server configuration and deployment. But, in
our case, we are going to use it to update our servers. This is a great solution for me since I already had some
familiarity with Ansible and it's a great tool to have in your toolbelt. I need to look up some Ansible tricks a
bit more to look at full server provisioning, but this will be a good start.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt; I am performing these steps with the assumption that you have already set up your Kamal servers and have
them running. You can check out a &lt;a href="https://www.driftingruby.com/episodes/kamal-in-github-actions"&gt;recent episode on Kamal&lt;/a&gt;
which covers deploying with Github Actions, but I also go through the steps of setting up the servers. I also have
an &lt;a href="https://www.driftingruby.com/episodes/deploying-with-mrsk"&gt;older episode when Kamal was called MRSK&lt;/a&gt; which
covers setting up the servers with a managed database and load balancer.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt; This Ansible playbook is also assuming that you'll be running this on a computer that has shell access
to the servers that you're wanting to manage.&lt;/p&gt;

&lt;h2 id="setting-up-ansible"&gt;Setting up Ansible&lt;/h2&gt;

&lt;p&gt;Since I am on an Apple computer, I'm going to use Homebrew to install Ansible. If you're on a different platform,
you can check out the &lt;a href="https://docs.ansible.com/ansible/latest/installation_guide/intro_installation.html"&gt;Ansible installation documentation&lt;/a&gt;
for more information.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;brew install ansible
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id="understanding-ansible"&gt;Understanding Ansible&lt;/h2&gt;

&lt;p&gt;This Ansible setup is going to have two different files. The first is the &lt;code&gt;inventory&lt;/code&gt; file which will contain
the list of servers that we want to manage. The second is the &lt;code&gt;playbook.yml&lt;/code&gt; file which will contain the
instructions for Ansible to run.&lt;/p&gt;

&lt;p&gt;The &lt;code&gt;inventory&lt;/code&gt; file will look something like this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[server_group]
app1            ansible_host=PUBLIC_IP_ADDRESS_OF_SERVER
app2            ansible_host=PUBLIC_IP_ADDRESS_OF_SERVER
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The inventory has different groups of servers. In this case, we only have one group called &lt;code&gt;server_group&lt;/code&gt;. This
group contains two servers. The first server is called &lt;code&gt;app1&lt;/code&gt; and the second server is called &lt;code&gt;app2&lt;/code&gt;. You can
add as many servers as you want to this group. You can also create multiple groups if you want to manage different
servers with different playbooks. You will want to replace the &lt;code&gt;PUBLIC_IP_ADDRESS_OF_SERVER&lt;/code&gt; with the public IP
address of your server. This should be the same IP address that you use to SSH into your server and that you
have set up in the Kamal &lt;code&gt;deploy.yml&lt;/code&gt; file.&lt;/p&gt;

&lt;p&gt;The &lt;code&gt;playbook.yml&lt;/code&gt; file will look something like this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;---
- name: Update and Upgrade Packages on Servers Sequentially
  hosts: all
  serial: 1
  become: yes

  tasks:
    - name: Run apt update and apt upgrade on servers
      apt:
        update_cache: yes
        upgrade: yes
        cache_valid_time: 3600
      register: apt_update

    - name: Pause for 2 minutes before next server update
      pause:
        minutes: 2
      when: apt_update.changed
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This is a bit more complicated because we have some additional things to take into consideration. The first
thing to note is that we are running the playbook on all servers in the inventory. This is because we want
to update all of the servers. However, we don't want to update them all at the same time. This is where the
&lt;code&gt;serial&lt;/code&gt; option comes in. This will tell Ansible to run the playbook on one server at a time. This is important
because we don't want to take down our application while we are updating the servers. This could happen if
there was an update to Docker. If we updated Docker on all of the servers at the same time, then our application
would be down until the update was complete. This is why we want to update the servers one at a time.&lt;/p&gt;

&lt;p&gt;The &lt;code&gt;become&lt;/code&gt; option tells Ansible to run the commands as the root user. This is important because we need to
be able to update the packages on the server. If we didn't have this option, then we would get an error when
trying to update the packages.&lt;/p&gt;

&lt;p&gt;The &lt;code&gt;tasks&lt;/code&gt; section is where we define the tasks that we want Ansible to run. The first task is to run the
&lt;code&gt;apt update&lt;/code&gt; and &lt;code&gt;apt upgrade&lt;/code&gt; commands. This will update all of the packages on the server. The &lt;code&gt;register&lt;/code&gt;
option tells Ansible to store the output of the command in a variable called &lt;code&gt;apt_update&lt;/code&gt;. This will be used
in the next task.&lt;/p&gt;

&lt;p&gt;The second task is to pause for 2 minutes before running the next server update. This is important because
we want to make sure that the server is fully updated before we update the next server. The &lt;code&gt;when&lt;/code&gt; option
tells Ansible to only run this task if the &lt;code&gt;apt_update&lt;/code&gt; variable has changed. This will only happen if the
server was updated. If the server was already up to date, then this task will not run.&lt;/p&gt;

&lt;h2 id="running-the-ansible-playbook"&gt;Running the Ansible Playbook&lt;/h2&gt;

&lt;p&gt;Now that we have our playbook and inventory files set up, we can run the playbook using the following command:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ansible-playbook -i inventory playbook.yml
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This will run the playbook on all of the servers in the inventory file. You should see something like this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;PLAY [Update and Upgrade Packages on Servers Sequentially] *********

TASK [Gathering Facts] *********************************************
ok: [app1]

TASK [Run apt update and apt upgrade on servers] *******************
ok: [app1]

TASK [Pause for 2 minutes before next server update] ***************
skipping: [app1]

PLAY [Update and Upgrade Packages on Servers Sequentially] *********

TASK [Gathering Facts] *********************************************
ok: [app2]

TASK [Run apt update and apt upgrade on servers] *******************
ok: [app2]

TASK [Pause for 2 minutes before next server update] ***************
skipping: [app2]

PLAY RECAP *********************************************************
app1                       : ok=2    changed=0    unreachable=0    failed=0    skipped=1    rescued=0    ignored=0
app2                       : ok=2    changed=0    unreachable=0    failed=0    skipped=1    rescued=0    ignored=0
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You can see that the playbook ran on both servers and that the &lt;code&gt;apt update&lt;/code&gt; and &lt;code&gt;apt upgrade&lt;/code&gt; commands were
run. You can also see that the &lt;code&gt;Pause for 2 minutes before next server update&lt;/code&gt; task was skipped. This is because
the server was already up to date.&lt;/p&gt;

&lt;p&gt;I hope that this helps close another gap in the Kamal deployment process. I'm looking forward to seeing what
else is in store for Kamal and I'm excited to see what the future holds for Ruby on Rails deployments.&lt;/p&gt;
</content>
  </entry>
  <entry>
    <title>Using SQLite in Production</title>
    <link rel="alternate" href="https://blog.driftingruby.com/articles/2023/10/01/using-sqlite-in-production.html"/>
    <id>https://blog.driftingruby.com/articles/2023/10/01/using-sqlite-in-production.html</id>
    <published>2023-09-30T22:46:00-04:00</published>
    <updated>2023-10-01T01:06:55-04:00</updated>
    <author>
      <name>Dave Kimura</name>
    </author>
    <content type="html">
&lt;p&gt;So there is a recent push on using SQLite in production, but there's also little talks around
the pros and cons of doing so. This article will try to cover the pros and cons of using SQLite
in production.&lt;/p&gt;

&lt;p&gt;To start, there's some value in asking ChatGTP what they think about using SQLite in production.
So, the below information is from ChatGPT and it has some interesting points. However, some of these
points are flawed and I don't agree with them. So, this is another example where you must take
everything with a grain of salt when it comes to generative AI content.&lt;/p&gt;

&lt;p&gt;My responses will be in red next to the points made by ChatGPT.&lt;/p&gt;

&lt;h2 id="pros-of-using-sqlite-in-production"&gt;Pros of Using SQLite in Production:&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Simplicity and Maintenance:&lt;/strong&gt; SQLite requires no separate server process or system to operate.
Its serverless nature makes it easy to set up, backup, and maintain.
&lt;span style="color:red"&gt;Keep in mind that this is requiring you to install the sqlite service
on the server that will be using the database. This isn't a big deal as the &lt;code&gt;sqlite3&lt;/code&gt; binary
is about 4MB. Though, it doesn't run as a service so it is simple to manage.&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Portability:&lt;/strong&gt; All the data in an SQLite database is stored in a single file, making it extremely
easy to transfer, copy, or backup.
&lt;span style="color:red"&gt;This is true. However, you may want to be careful in situations where
you could run out of disk space. Make sure that you have proper monitoring in place on the server. Also,
while I've never confirmed that this could be an issue, but this is also putting all of your eggs in
one basket. If there was ever an issue with the file being corrupted then it could be a total loss.
Again, I've never seen this happen, but I also don't use SQLite enough to have experienced this.&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Low Resource Footprint:&lt;/strong&gt; It’s lightweight and requires minimal memory, making it suitable for
devices with limited resources like IoT devices or mobile applications.
&lt;span style="color:red"&gt;The IoT devices and mobile applications is where I have seen this the
most. While a default Ruby on Rails application does come with SQLite as the default database
engine, typically it isn't seen in a production setting.&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Consistent Performance:&lt;/strong&gt; For applications with a moderate load and where the read operations far
outnumber the write operations, SQLite can deliver consistent and fast performance.
&lt;span style="color:red"&gt;There's no network hops to worry about. Optimized SQL queries are often
not the slow point of the application. Even with network latency it's often a tiny amount such that
you won't notice. If we're talking about a huge application that's service millions of requests
per second then these network hops could add up with a traditional database.&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;ACID Compliant:&lt;/strong&gt; SQLite transactions are ACID compliant, ensuring data integrity even after
system crashes or power failures.
&lt;span style="color:red"&gt;Atomic, consistent, isolated, and durable (ACID). It's true that SQLite
has come a long way in terms of integrity and reliability of handling the data. However, with
system crashes or power failures, database corruptions can occur especially on systems where RAID
systems are configured improperly. If the RAID cache is set to write-back, you get better performance
but could end up in a situation where there is data loss or corruption.&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Cross-Platform:&lt;/strong&gt; SQLite is cross-platform and can be used on all major operating systems,
including Windows, Linux, and macOS.
&lt;span style="color:red"&gt;In terms of deploying a Ruby on Rails application, this is kind of a moot
point as we will typically not change the platform the application is hosted on.&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Free and Open-Source:&lt;/strong&gt; SQLite is free to use and is released under the Public Domain license,
which means it can be used for any purpose, including commercial applications.
&lt;span style="color:red"&gt;The same could be said for MySQL (or MariaDB) and PostgreSQL. MySQL is GPL licensed
which probably doesn't make a difference if you're hosting your own application and not distributing it.&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;No Configuration:&lt;/strong&gt; SQLite doesn’t require any configuration or setup, making it easy to
use for beginners.
&lt;span style="color:red"&gt;Yes, properly configuring a MySQL or PostgreSQL instance can be difficult
and should be done properly for optimal performance. However, using a managed service with AWS,
GCP, Digital Ocean, Azure, etc. will often have a decent configuration that you don't need to
worry about.&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;No Dependencies:&lt;/strong&gt; SQLite doesn’t have any dependencies, which means it can be used in any
programming language or framework.
&lt;span style="color:red"&gt;While MySQL and PostgreSQL have dependencies and you'll need to install
some libraries on the server, they're well documented steps and usually not a big deal.&lt;/span&gt;&lt;/p&gt;

&lt;h2 id="cons-of-using-sqlite-in-production"&gt;Cons of Using SQLite in Production:&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Concurrency Issues:&lt;/strong&gt; SQLite is not ideal for applications with a high level of concurrent write
operations. It locks the entire database during a write operation, which can cause a bottleneck.
&lt;span style="color:red"&gt;If we're working on a single threaded application since we're talking about
smaller applications without too much traffic, this will not really be an issue. However, it does
bring up a good point for any kind of background workers that occur asynchronous. Even though
SQLite does a good job at handling transactions, it is easy to get ourselves into situations of
Deadlocks when background jobs are trying to write to the database at the same time.&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Not Suited for Large Databases:&lt;/strong&gt; While SQLite supports database files up to 140 TB, it’s generally
not the best choice for very large datasets due to potential performance issues.
&lt;span style="color:red"&gt;Most databases will never reach 140TB. But this does not talk to file system limits.
This is really a moot point since modern file systems exceed SQLite's limits, but if you're file system
was improperly set (to something like Fat32), then you could easily reach the 4GB limit. Most likely,
if your database was even getting to a fraction of this size, you'll have other issues to be concerned with;
system resources, performance issues, etc.&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Limited Built-in Features:&lt;/strong&gt; Unlike more robust RDBMS like PostgreSQL or MySQL, SQLite doesn’t
have a wide range of built-in tools or support for stored procedures, right out of the box.
&lt;span style="color:red"&gt;It supports the important ones like JSON data types. Typically, we don't use
stored procedures in Ruby on Rails applications since the ActiveRecord ORM is powerful enough in most cases.&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Limited Security:&lt;/strong&gt; SQLite doesn’t have built-in support for user management or access control,
making it less secure than other RDBMS.
&lt;span style="color:red"&gt;While this can be a critical aspect in some scenarios, the Ruby on Rails applications
will likely not need this level of access control. However, it does speak to the vulnerability of the database
as it will be much easier to make a copy of if the system is ever breached. In a traditional deployment, there's
still nothing that could really stop the attacker if they gained access to the machine, but it does offer
some levels of obfuscation if they don't know about how Rails handles credentials. But, the reality is that
if a bad actor has gained access to your machine, all bets are already off and you're in a very bad situation.&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;No Client-Server Model:&lt;/strong&gt; SQLite doesn’t have a client-server model, which means it can’t be
accessed by multiple users at the same time.
&lt;span style="color:red"&gt;This is going to be a big issue if High Availability is a concern. Typically, we will
have a load balancer as the SSL termination point and it will route the traffic to one of the available web
servers. This is a deal breaker in many situations as using SQLite in a production setting could mean that
you have downtime during deployments. You lose the ability for rolling deployments and have basically put
all eggs in one basket. Depending on your deployment strategy, it could also mean that you're having to maintain
a server instead of being able to manually or automatically provision and scale up new web servers.&lt;/span&gt;&lt;/p&gt;

&lt;h2 id="conclusion"&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;So, there's some pros and cons to using SQLite in production. I think that the pros are valid, but the cons
must also be taken into consideration. The recent push for using SQLite in production isn't necessarily a bad
direction, but shouldn't be blindly followed. There's a lot of factors that go into deciding what database
engine to use. I think that SQLite is a great choice for smaller applications that don't have a lot of traffic.
It is also a good choice when architecture simplicity is a concern. Just make sure that you're weighing all of
your options and understand the consequences and benefits of your choices. There's no one size fits all solution
in this case. Also keep in mind that a lot of platforms will have a free tier or a cheap tier for managed database
engines. You can get a lot of value out of these services and it will be much easier to scale up in the future
if you need to. I think that the biggest concern is the lack of High Availability This is a deal breaker for most
applications that are going to be deployed in a production setting. However, for non-revenue generating applications
or applications that are not mission critical, SQLite could be a good choice.&lt;/p&gt;
</content>
  </entry>
  <entry>
    <title>Running Meilisearch</title>
    <link rel="alternate" href="https://blog.driftingruby.com/articles/2023/05/06/running-meilisearch.html"/>
    <id>https://blog.driftingruby.com/articles/2023/05/06/running-meilisearch.html</id>
    <published>2023-05-05T20:00:00-04:00</published>
    <updated>2023-09-30T22:29:56-04:00</updated>
    <author>
      <name>Dave Kimura</name>
    </author>
    <content type="html">&lt;p&gt;MeiliSearch is a powerful, fast, and easy-to-use search engine, perfect for developers who want to implement search functionality into their applications. In this blog post, we will walk you through the process of deploying MeiliSearch with SSL encryption using Docker Compose and Nginx as a reverse proxy. This setup ensures a secure connection between your users and your MeiliSearch instance.&lt;/p&gt;

&lt;h2 id="prerequisites"&gt;Prerequisites&lt;/h2&gt;

&lt;p&gt;Before you begin, make sure you have the following tools installed on your system:&lt;/p&gt;

&lt;p&gt;Docker: Ensure Docker is installed and running on your machine. You can follow the installation guide on the official Docker website.&lt;/p&gt;

&lt;p&gt;Docker Compose: Make sure Docker Compose is installed. You can find installation instructions on the official Docker Compose website.&lt;/p&gt;

&lt;p&gt;OpenSSL: Required for generating SSL certificates. You can install OpenSSL through your package manager or download it from the official OpenSSL website.&lt;/p&gt;

&lt;h2 id="configuration"&gt;Configuration&lt;/h2&gt;

&lt;p&gt;Create a docker-compose.yml file in your project directory and copy the following content:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;version: '3.8'

services:
  meilisearch:
    image: getmeili/meilisearch:v1.1
    restart: always
    ports:
      - '7700:7700'
    environment:
      - MEILI_MASTER_KEY=key
    volumes:
      - meili_data:/meili_data

  nginx:
    image: nginx:stable-alpine
    restart: always
    ports:
      - '80:80'
      - '443:443'
    volumes:
      - ./nginx.conf:/etc/nginx/nginx.conf
      - /home/ubuntu/cert.pem:/etc/nginx/certs/fullchain.pem
      - /home/ubuntu/key.pem:/etc/nginx/certs/privkey.pem
    depends_on:
      - meilisearch

volumes:
  meili_data:
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This configuration file sets up two services: MeiliSearch and Nginx.&lt;/p&gt;

&lt;h2 id="meilisearch-service"&gt;Meilisearch service:&lt;/h2&gt;

&lt;p&gt;You should replace the key value with a random string. This key will be used to secure your MeiliSearch instance. You can generate a random string using the following command:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;openssl rand -hex 16
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id="nginx-service"&gt;Nginx service:&lt;/h2&gt;

&lt;p&gt;There's a bit more config to do with the Nginx service. We can create a ssl certificate using OpenSSL. We will use this certificate to enable SSL encryption for our MeiliSearch instance. You can generate a self-signed certificate using the following command:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;openssl req -x509 -newkey rsa:4096 -keyout key.pem -out cert.pem -days 365 -subj "/CN=yourdomainname" -nodes
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Then we can create a configuration file called nginx.conf and copy the following content:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;events {
    worker_connections 1024;
}
http {
    server {
        listen 80;
        server_name yourdomainname;
        return 301 https://$host$request_uri;
    }
    server {
        listen 443 ssl;
        server_name yourdomainname;
        ssl_certificate /etc/nginx/certs/fullchain.pem;
        ssl_certificate_key /etc/nginx/certs/privkey.pem;
        ssl_protocols TLSv1.2 TLSv1.3;
        ssl_ciphers 'TLS_AES_128_GCM_SHA256:TLS_AES_256_GCM_SHA384:TLS_CHACHA20_POLY1305_SHA256:ECDHE-RSA-AES128-GCM-SHA256:ECDHE-RSA-AES256-GCM-SHA384';
        location / {
            proxy_pass http://meilisearch:7700;
            proxy_set_header Host $host;
            proxy_set_header X-Real-IP $remote_addr;
            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
            proxy_set_header X-Forwarded-Proto $scheme;
        }
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You'll want to update the appropriate values for your domain name and SSL certificate paths.&lt;/p&gt;

&lt;h2 id="running-the-application"&gt;Running the application&lt;/h2&gt;

&lt;p&gt;Now that we have our configuration files ready, we can run the application using the following command:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;docker compose up -d
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This will start both services in the background. You can check the status of your services using the following command:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;docker compose ps
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You should see something like this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Name                Command               State           Ports
--------------------------------------------------------------------------------
meilisearch         ...                   Up              ..
nginx               ...                   Up              ..
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The &lt;code&gt;-d&lt;/code&gt; in the &lt;code&gt;docker compose up -d&lt;/code&gt; command tells Docker to run the services in the background.&lt;/p&gt;

&lt;p&gt;This will also cause the services to restart automatically if they crash or if you reboot the machine.&lt;/p&gt;

&lt;p&gt;Thanks for reading!&lt;/p&gt;
</content>
  </entry>
  <entry>
    <title>Ruby 3.2.1 with YJIT</title>
    <link rel="alternate" href="https://blog.driftingruby.com/articles/2023/03/04/ruby-with-yjit.html"/>
    <id>https://blog.driftingruby.com/articles/2023/03/04/ruby-with-yjit.html</id>
    <published>2023-03-03T19:00:00-05:00</published>
    <updated>2023-03-04T23:34:53-05:00</updated>
    <author>
      <name>Dave Kimura</name>
    </author>
    <content type="html">&lt;p&gt;I've been using Ruby 3.2.1 + YJIT for some time now and overall, I'm seeing anywhere from 10-20%
performance improvements over running Ruby 3.0.5. Sorry, I don't have an apple-to-apple
comparison for just YJIT vs non-YJIT. However, the performance improvements are real world
scenarios on a Ruby on Rails 7.0.4 application.&lt;/p&gt;

&lt;p&gt;So, I've recently formatted my primary desktop which is running macOS 13.2.1 and figured that
I would document my steps on my preferred way of installing Ruby as well as getting YJIT to work.
It's important to note that this is also an Apple Silicon machine. These steps should work regardless
of which Apple Silicon you're using.&lt;/p&gt;

&lt;p&gt;First, I'll install homebrew. This will automatically install the Command Line Tools for Xcode.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;/bin/bash -c "$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)"
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You'll then be prompted to run some additional commands.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;(echo; echo 'eval "$(/opt/homebrew/bin/brew shellenv)"') &amp;gt;&amp;gt; /Users/$(whoami)/.zprofile
eval "$(/opt/homebrew/bin/brew shellenv)"
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We can then start our install of Ruby. However, there are a few things that we should do first.
Firstly, we need to select a manager for our Ruby interpreter. I normally use RVM as it's what
I used for the longest time. It works well and I never had any complaints. However, with Ruby
3.2.1, we're also going to need an installation of Rust if we want to enable YJIT. So, instead
of RVM, I'm going to be using ASDF.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;brew install asdf
echo -e "\n. $(brew --prefix asdf)/libexec/asdf.sh" &amp;gt;&amp;gt; ${ZDOTDIR:-~}/.zshrc
source ~/.zshrc
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now that we have a manager for our Ruby and Rust, we can now proceed to install Rust.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;asdf plugin add rust
asdf install rust latest
asdf global rust latest
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now that Rust is installed, we can proceed with our Ruby installation. Normally there are some
dependencies that we need to handle, but luckily, asdf will take care of that for us.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;asdf plugin add ruby
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;However, there is a strange thing with the mapping of the libraries so if we were to try and
install Ruby right now, it would fail. So let's take care of this. This is around the libyaml
package and we need to set some environment variables for this. I've added all four of these
to my ~/.zshrc so that Ruby can install properly and YJIT is enabled out of the box.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;brew install openssl@3 readline libyaml gmp
echo 'export LDFLAGS="-L$(brew --prefix libyaml)/lib"' &amp;gt;&amp;gt; ~/.zshrc
echo 'export CPPFLAGS="-I$(brew --prefix libyaml)/include"' &amp;gt;&amp;gt; ~/.zshrc
echo 'export RUBY_YJIT_ENABLE=1' &amp;gt;&amp;gt; ~/.zshrc
echo 'export RUBY_CONFIGURE_OPTS=--enable-yjit' &amp;gt;&amp;gt; ~/.zshrc
source ~/.zshrc
asdf install ruby 3.2.1
asdf global ruby 3.2.1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;I had to restart my terminal, but once I did, everything seemed to work as expected.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ruby -v
ruby 3.2.1 (2023-02-08 revision 31819e82c8) +YJIT [arm64-darwin22]
&lt;/code&gt;&lt;/pre&gt;
</content>
  </entry>
  <entry>
    <title>Pushing Code to Multiple Repositories with Git</title>
    <link rel="alternate" href="https://blog.driftingruby.com/articles/2023/02/23/pushing-to-multiple-repositories.html"/>
    <id>https://blog.driftingruby.com/articles/2023/02/23/pushing-to-multiple-repositories.html</id>
    <published>2023-02-22T19:00:00-05:00</published>
    <updated>2023-02-23T11:30:44-05:00</updated>
    <author>
      <name>Dave Kimura</name>
    </author>
    <content type="html">&lt;p&gt;Have you ever found yourself needing to publish your code to multiple repositories? Perhaps you have a static site hosted on Github Pages and want to have multiple domains point to it. Or maybe you manage multiple technology-related blogs and need to consolidate your content into a single blog, while still updating it on multiple domain names. In this article, we will explore a solution to push code to multiple repositories with minimal effort using Git.&lt;/p&gt;

&lt;p&gt;As mentioned earlier, there are scenarios where you need to update your code on multiple repositories, and doing so manually can be time-consuming and error-prone. For instance, you may have a blog that you want to host on multiple domains, but instead of redirecting users to a new site, you prefer to update the content on both domains simultaneously. In such cases, you can use Git to push changes to multiple repositories with just a few commands.&lt;/p&gt;

&lt;p&gt;To push code to multiple repositories, you first need to initialize your Git repository and push the changes to one of the repositories. In our example, let's say we push the changes to the repository kobaltz/blog-kobaltz.git. Now, we want to push the same code to another repository kobaltz/blog-driftingruby.git. To achieve this, we can add another push URL to our .git/config file using the following command:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;git remote set-url --add --push origin git@github.com:kobaltz/blog-driftingruby.git
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This command adds a new push URL to the origin remote and sets it to the git@github.com:kobaltz/blog-driftingruby.git repository. Now, whenever you commit your changes and push them, Git will push the changes to both repositories.&lt;/p&gt;

&lt;p&gt;You can verify that the push URL has been added by running the following command:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;git remote -v
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This command will display the two push URLs under the origin remote, as shown below:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;origin  git@github.com:kobaltz/blog-kimura.git (fetch)
origin  git@github.com:kobaltz/blog-kimura.git (push)
origin  git@github.com:kobaltz/blog-driftingruby.git (push)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now, your code is being pushed to two repositories with minimal effort. If you ever want to remove one of the push URLs, you can do so by using the following command:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;git remote set-url --delete --push origin git@github.com:kobaltz/blog-driftingruby.git
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;In this article, we've explored a simple solution to push code to multiple repositories using Git. By adding a new push URL to the origin remote, we can push changes to multiple repositories with minimal effort. This technique can be useful in scenarios where you need to update your code on multiple domains or repositories. And, remember that you can use different services as well for the push URLs; one pointing to
GitHub and another pointing to GitLab.&lt;/p&gt;

&lt;p&gt;By consolidating your code base and keeping multiple repositories up to date, you can simplify your workflow and save time. Give this approach a try and see how it works for you. Happy coding!&lt;/p&gt;
</content>
  </entry>
  <entry>
    <title>Docker Builds are slow on M1</title>
    <link rel="alternate" href="https://blog.driftingruby.com/articles/2022/04/09/docker-builds-are-slow-on-m1.html"/>
    <id>https://blog.driftingruby.com/articles/2022/04/09/docker-builds-are-slow-on-m1.html</id>
    <published>2022-04-08T20:00:00-04:00</published>
    <updated>2023-02-18T01:05:28-05:00</updated>
    <author>
      <name>Dave Kimura</name>
    </author>
    <content type="html">&lt;p&gt;This is a neat Docker trick for those who have an ARM development machine (Apple M1), but sometimes need to build x86/amd64 images locally to push up to a registry.&lt;/p&gt;

&lt;p&gt;Sure, having a CI/CD platform to do this is probably ideal, but for little programs or just sometimes in general, it may be handy to know this.&lt;/p&gt;

&lt;p&gt;If you have a spare Intel machine laying around, you can turn it into a build server. Get something like &lt;a href="https://docs.docker.com/engine/install/ubuntu/"&gt;Ubuntu installed&lt;/a&gt; on there and install the latest Docker. On your development machine, &lt;a href="https://www.ssh.com/academy/ssh/copy-id"&gt;copy your publish SSH key over to this build server.&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;So now you you have your server set up and you want to build your Docker image for an x86/amd64 platform. Typically, you would run a command like this below to target the platform.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;docker buildx build -f Dockerfile --platform linux/amd64 .
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;And this will work, but what you'll notice is that this is an extremely slow process. The Apple Silicon chips are amazing and are the fastest machines I've ever used. However, when emulating the x86 instructions to build a docker image, it takes such a long time. I've seen this take over an hour on larger and more complex projects.&lt;/p&gt;

&lt;p&gt;Just as an example. Here we have a very simple Ruby on Rails application that has little moving parts. I'm using things like &lt;code&gt;esbuild&lt;/code&gt; and &lt;code&gt;css-bundling&lt;/code&gt;, but nothing fancy. On the M1 chip, it took over 250 seconds to build the image.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# Apple Silicon
[+] Building 316.4s (23/23) FINISHED
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;However, on a AMD 5900X server, I have a Virtual Machine running on there which has Docker installed. Running the exact same project on there took much less time.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# AMD 5900X
[+] Building 62.3s (24/24) FINISHED
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;So, the main concern here is that I do not want to interrupt my normal process on how I build images or handle things. It would be a pain to push up my code, ssh into the build server, pull it down, and then build the image. This is a lot of steps, but luckily there is a MUCH easier way to do this.&lt;/p&gt;

&lt;p&gt;Docker's buildx build command has a flag that we can specify a specific builder.&lt;/p&gt;

&lt;p&gt;So, we can create this builder on our local machine. The nice part about this creation is that it is idempotent, so you can run this command many times without changing the result. All we have to do is to create a builder profile and in this case I have named it &lt;code&gt;amd64_builder&lt;/code&gt;.  Since this builder is a pool of resources, I give a name to for the VM. I'm also specifying the platform that I'm building against and then pass in the SSH url for my builder machine.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;docker buildx create \
  --name amd64_builder \
  --node linux_amd64_builder \
  --platform linux/amd64 \
  ssh://USERNAME@IP_ADDRESS_OF_BUILDER
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now, I can build and push the image to the registry. The magical flag that we'll use is &lt;code&gt;--builder&lt;/code&gt; as we can now specify the builder VM. The rest of the buildx command is the same as we would expect.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;docker buildx build \
  --builder amd64_builder \
  --tag USERNAME/REPONAME:TAG \
  --push .
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;In some cases, I've seen this go almost 10x faster than the amd64 emulation on the M1 chip. If you have a spare Intel/AMD machine laying around, this may be a worthwhile adventure.&lt;/p&gt;
</content>
  </entry>
</feed>
